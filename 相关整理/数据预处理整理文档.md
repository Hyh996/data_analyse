# 数据预处理（1.3）
https://awrmjmi97zs.feishu.cn/sync/D7GkdC0z2sWzsTbY9rictmNsnjd
暂时无法在飞书文档外展示此内容
整合涉及知识库内文档：
数据预处理、pandas库、实例：数据预处理+文件拆分、数据分析项目（野蛮时代）、数据分析项目（北京租房数据分析）、阅读笔记整理（《利用python进行数据分析（第二版）》）
更多补充参考：
https://blog.csdn.net/xiewenrui1996/article/details/109055070
https://blog.csdn.net/Jormungand_V/article/details/109813368
https://blog.csdn.net/MsSpark/article/details/83154128
https://github.com/luanshiyinyang/DataMining

---
由于该文档整合自多个文档且大部分为直接引用的同步块，在变量命名上会存在混淆和模糊，最好是直接每个标题分开看。完整实例那里进行了一次梳理，可以结合着来看
前期准备
固定起手式
'''
# 必备
import pandas as pd
# 一般情况下encoding='gbk'或'utf-8'，实在不行上网查查改其他的，或者直接将原来的文档另存为csv utf-8的格式
file = pd.read_csv(r'对应文件的路径.csv',encoding='gbk')
下图截取自《利用python进行数据分析（第二版）》
[图片]
备选常用
# 一般情况下只有需要数值运算处理数据时才需要
import numpy as np
# 导出进数据库时需要，用于连接和操作数据库
from sqlalchemy import create_engine
处理主体
常用函数
暂时无法在飞书文档外展示此内容
排序
# 按照日期排序
file['日期'] = pd.to_datetime(file['日期'])
file = file.sort_values(by='日期')
'''
sort_values()函数用途
pandas中的sort_values()函数原理类似于SQL中的order by，可以将数据集依照某个字段中的数据进行排序，该函数即可根据指定列数据也可根据指定行的数据排序。
DataFrame.sort_values(by=‘##’,axis=0,ascending=True, inplace=False, na_position=‘last’)
参数说明：
by：指定列名(axis=0或’index’)或索引值(axis=1或’columns’)
axis：若axis=0或’index’，则按照指定列中数据大小排序；若axis=1或’columns’，则按照指定索引中数据大小排序，默认axis=0
ascending：是否按指定列的数组升序排列，默认为True，即升序排列
inplace：是否用排序后的数据集替换原来的数据，默认为False，即不替换
na_position：{‘first’,‘last’}，设定缺失值的显示位置
空值处理
# 检查是否存在空值
print(file.isnull().sum())
# 缺失值处理：直接删除缺失值所在行，并重置索引
# print(data.isnull().sum())
data.dropna(axis=0, inplace=True)
data.reset_index(drop=True, inplace=True)
https://awrmjmi97zs.feishu.cn/sync/UEOndrKSus9uxVbAepCcnXkInIc
https://awrmjmi97zs.feishu.cn/sync/ZFTNdTGIzsx9flbm1yWc8Lb5n4g
https://awrmjmi97zs.feishu.cn/sync/RenSdvpcWshOYEbqeXYc2Mfgnbe
重置索引reset_index（）
重置数据帧的索引，并使用默认索引。如果数据帧具有多重索引，则此方法可以删除一个或多个level。
reset_index(level=None, drop=False, inplace=False, col_level=0, col_fill='')
参数说明
level：可以是int, str, tuple, or list, default None等类型。作用是只从索引中删除给定级别。默认情况下删除所有级别。
drop：bool, default False。不要尝试在数据帧列中插入索引。这会将索引重置为默认的整数索引。
inplace：bool, default False。修改数据帧（不要创建新对象）。
col_level：int or str, default=0。如果列有多个级别，则确定将标签插入到哪个级别。默认情况下，它将插入到第一层。
col_fill：object, default。如果列有多个级别，则确定其他级别的命名方式。如果没有，则复制索引名称。
返回：
DataFrame or None。具有新索引的数据帧，如果inplace=True，则无索引。
当然还有同时存在两个表，可以互补缺失数据的情况
该例子截取自《利用python进行数据分析（第二版）》
https://awrmjmi97zs.feishu.cn/sync/YZI5dJ55osh1frbROKJcxRtJndh
重复值处理
#  数据重复处理: 删除重复值
print(data[data.duplicated()])
data.drop_duplicates(inplace=True)
data.reset_index(drop=True, inplace=True) # 同理以上重置索引内容
https://awrmjmi97zs.feishu.cn/sync/Lnt1dc5oqsRKxxbmxdwcrOjXn0b
异常值处理
# 去除异常值
file = file.loc[file['货量']<80000]
# 异常值清洗
data['户型'].unique()
# print(data[data['户型'] == '户型'])
data = data[data['户型'] != '户型']
https://awrmjmi97zs.feishu.cn/sync/Mol7dZvx4sPXjrbUrMocMi7Nnxh
插值处理
以下内容直接引自链接中的项目内容
https://github.com/luanshiyinyang/DataMining
# 该差值处理程序出处：https://github.com/luanshiyinyang/DataMining/blob/master/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/%E6%8F%92%E5%80%BC%E5%A4%84%E7%90%86.py

import pandas as pd
from scipy.interpolate import lagrange

inputFile = './catering_sale.xls'
outputFile = './sales.xls'

data = pd.read_excel(inputFile)
# 异常值置为空
data[u'销量'][(data[u'销量'] < 400) | (data[u'销量'] > 5000)] = None

# 自定义列向量插值函数
# s为列向量，n为被插值的位置，k为取前后的数据个数，默认为5


def ployinterp_column(s, n, k=5):
    y = s[list(range(n-k, n)) + list(range(n+1, n+1+k))]
    y = y[y.notnull()]
    return lagrange(y.index, list(y))(n)


# 逐个元素判断是否需要插值
for i in data.columns:
    for j in range(len(data)):
        if (data[i].isnull())[j]:
            data[i][j] = ployinterp_column(data[i], j)
data.to_excel(outputFile)
列替换
# 清洗，列替换
data.loc[:, '地铁'] = data['地铁'].apply(lambda x: x.replace('地铁：', ''))
增加列
# 增加列
data.loc[:, '所在楼层'] = data['楼层'].apply(lambda x: int(x.split('/')[0]))
data.loc[:, '总楼层'] = data['楼层'].apply(lambda x: int(x.replace('层', '').split('/')[-1]))
data.loc[:, '地铁数'] = data['地铁'].apply(lambda x: len(re.findall('线', x)))
data.loc[:, '距离地铁距离'] = data['地铁'].apply(lambda x: int(re.findall('(\d+)米', x)[-1]) if re.findall('(\d+)米', x) else -1)
转换数据类型
可以先使用df.dtypes查看属性
# 查看属性
print(df.dtypes)
# 数据类型转换
data['价格'] = data['价格'].astype(np.int64)
data['面积'] = data['面积'].astype(np.int64)
data['距离地铁距离'] = data['距离地铁距离'].astype(np.int64)
通过astype()方法强制转换数据的类型
astype(dypte, copy=True, errors = ‘raise’, **kwargs)
参数说明：
- dtype：表示数据类型，例如np.int32,np.float64等
- copy：是否建立副本，默认为True
- errors：错误采取的处理方式，可以取值为raise或ignore，默认为raise。其中raise表示允许引发异常，ignore表示抑制异常。
https://awrmjmi97zs.feishu.cn/sync/MOMGd195Ms3oOubOrDDcFbXinlc
规范化处理
同样直接同异常值插值处理部分，直接引用自同一项目。由于之前建模一直用的是R，规范化处理之前都是直接用R完成的，这个地方等后面有时间再内化补充
data = pd.read_excel(file)
# 最小-最大规范化
print((data - data.min())/(data.max() - data.min()) )
# 零-均值规范化
print((data - data.mean())/data.std())
# 小数定标规范化
print(data/10**np.ceil(np.log10(data.abs().max())))
补充：R中规范化处理我自己常用的是直接将数值转为-1~1的方法
# 数据标准化处理
s<-scale(w[,-2])
class(s)
s<-data.frame(s,index=c("area","year"))
离散化处理
同上异常值插值处理部分，直接引用自同一项目。这里还有点没有完全明白，暂时先不动，等内化了再调整内容
# -*- coding: utf-8 -*-
# 直接引自：https://github.com/luanshiyinyang/DataMining/blob/master/%E6%95%B0%E6%8D%AE%E9%A2%84%E5%A4%84%E7%90%86/%E7%A6%BB%E6%95%A3%E5%8C%96%E5%A4%84%E7%90%86.py

# 数据规范化
import pandas as pd
from sklearn.cluster import KMeans

datafile = './discretization_data.xls'
data = pd.read_excel(datafile)
data = data[u'肝气郁结证型系数'].copy()
k = 4
# 等宽离散化
d1 = pd.cut(data, k, labels=range(k))

# 等频率离散化
w = [1.0 * i / k for i in range(k + 1)]
w = data.describe(percentiles=w)[4:4 + k + 1]  # 使用describe函数自动计算分位数
w[0] = w[0] * (1 - 1e-10)
d2 = pd.cut(data, w, labels=range(k))
kmodel = KMeans(n_clusters=k, n_jobs=4)  # 建立模型，n_jobs是并行数，一般等于CPU数较好
kmodel.fit(data.values.reshape((len(data), 1)))  # 训练模型
c = pd.DataFrame(kmodel.cluster_centers_).sort_values(0)  # 输出聚类中心，并且排序（默认是随机序的）
w = c.rolling(2).mean().iloc[1:]  # 相邻两项求中点，作为边界点
w = [0] + list(w[0]) + [data.max()]  # 把首末边界点加上
d3 = pd.cut(data, w, labels=range(k))


def cluster_plot(d, k):  # 自定义作图函数来显示聚类结果
    import matplotlib.pyplot as plt
    plt.rcParams['font.sans-serif'] = ['SimHei']  # 用来正常显示中文标签
    plt.rcParams['axes.unicode_minus'] = False  # 用来正常显示负号

    plt.figure(figsize=(8, 3))
    for j in range(0, k):
        plt.plot(data[d == j], [j for i in d[d == j]], 'o')

    plt.ylim(-0.5, k - 0.5)
    return plt


cluster_plot(d1, k).show()

cluster_plot(d2, k).show()
cluster_plot(d3, k).show()
收尾
主要是数据处理完事后使用（文件处理那个地方每个都附带文件导出的处理），一般要么导出csv或xsld，要么导入进数据库
文件导出
# 保存清洗后的数据 csv
data.to_csv('保存路径/文件名称.csv', index=False)
导入数据库
# 保存清洗后的数据 mysql
engine = create_engine('mysql://root:root@172.16.122.25:3306/test?charset=utf8')
data.to_sql('age_of_barbarians', con=engine, index=False, if_exists='append')
https://awrmjmi97zs.feishu.cn/sync/VxIsdmcpIsXUJwbHcNLc3Nq3ngc
[图片]
完整实例
常用模板
仅包含导入，排序，数据类型转换，处理空值、重复值、异常值几个常用数据清洗步骤，处理步骤直接用块注释处理了，需要时再使用ctrl+/取消块注释
import pandas as pd
import numpy as np
from sqlalchemy import create_engine

# 一般情况下encoding='gbk'或'utf-8'，实在不行上网查查改其他的，或者直接将原来的文档另存为csv utf-8的格式
data = pd.read_csv(r'对应文件的路径.csv',encoding='gbk')

# 数据类型转换
# 查看属性
print(data.dtypes)
# # 转换int
# data['需要转换的列'] = data['需要转换的列'].astype(np.int64)

# # 按照某一列排序
# data['指定列'] = pd.to_datetime(data['指定列'])
# data = data.sort_values(by='指定列')

# 检查是否存在空值
print(data.isnull().sum())
# # 缺失值处理：直接删除缺失值所在行，并重置索引
# data.dropna(axis=0, inplace=True)
# data.reset_index(drop=True, inplace=True)

# 数据重复处理: 删除重复值
# 检查重复值
print(data[data.duplicated()])
# # 保留第一个出现的重复值
# data.drop_duplicates(keep = 'first',inplace=True)
# data.reset_index(drop=True, inplace=True)

# # 去除异常值
# data = data.loc[data['要处理的那一列']<设置限制值]

# 保存清洗后的数据 csv，同理使用to_excel保存.xlsx
data.to_csv('对应文件的路径/保存文件名.csv', index=False)

# 保存清洗后的数据 mysql
engine = create_engine('mysql://用户名:密码@地址:端口号/数据库名称?charset=utf8')
data.to_sql('要保存的名称', con=engine, index=False, if_exists='append')
